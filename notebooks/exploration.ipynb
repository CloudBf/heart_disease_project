{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Configuration basique de matplotlib (sans style seaborn)\n",
    "plt.figure(figsize=(10, 6))\n",
    "%matplotlib inline\n",
    "\n",
    "# Chargement des données\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "\n",
    "# Séparation des features et de la cible\n",
    "X = heart_disease.data.features\n",
    "y = heart_disease.data.targets\n",
    "\n",
    "# Affichage des premières lignes\n",
    "print(\"Aperçu des données :\")\n",
    "print(X.head())\n",
    "\n",
    "# Informations sur le dataset\n",
    "print(\"\\nInformations sur le dataset :\")\n",
    "print(X.info())\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"\\nStatistiques descriptives :\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de l'âge\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=X, x='age', bins=30)\n",
    "plt.title('Distribution de l\\'âge des patients')\n",
    "plt.xlabel('Âge')\n",
    "plt.ylabel('Nombre de patients')\n",
    "plt.show()\n",
    "\n",
    "# Statistiques sur l'âge\n",
    "print(\"Statistiques sur l'âge :\")\n",
    "print(f\"Âge moyen : {X['age'].mean():.2f} ans\")\n",
    "print(f\"Âge médian : {X['age'].median():.2f} ans\")\n",
    "print(f\"Écart-type : {X['age'].std():.2f} ans\")\n",
    "print(f\"Âge minimum : {X['age'].min()} ans\")\n",
    "print(f\"Âge maximum : {X['age'].max()} ans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifions les valeurs uniques présentes dans y\n",
    "print(\"Valeurs uniques dans la variable cible :\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Vérifions les valeurs uniques pour le sexe\n",
    "print(\"\\nValeurs uniques pour le sexe :\")\n",
    "print(X['sex'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du DataFrame combiné\n",
    "df = X.copy()\n",
    "df['target'] = y\n",
    "\n",
    "# Création du mapping pour le sexe et la cible\n",
    "sex_mapping = {0: 'Femme', 1: 'Homme'}\n",
    "target_mapping = {0: 'Pas de maladie', 1: 'Maladie cardiaque'}\n",
    "\n",
    "# Application des mappings\n",
    "df['sex_label'] = df['sex'].map(sex_mapping)\n",
    "df['target_label'] = df['target'].map(target_mapping)\n",
    "\n",
    "# Création du graphique\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Création du tableau croisé avec les labels\n",
    "sex_disease = pd.crosstab(df['sex_label'], df['target_label'])\n",
    "\n",
    "# Visualisation avec des couleurs appropriées\n",
    "ax = sex_disease.plot(kind='bar', stacked=True, \n",
    "                     color=['#2ecc71', '#e74c3c'])  # Vert pour sain, rouge pour maladie\n",
    "\n",
    "# Personnalisation du graphique\n",
    "plt.title('Distribution des maladies cardiaques selon le sexe', \n",
    "          fontsize=14, pad=20)\n",
    "plt.xlabel('Sexe', fontsize=12)\n",
    "plt.ylabel('Nombre de patients', fontsize=12)\n",
    "plt.legend(title='Diagnostic', bbox_to_anchor=(1.05, 1), \n",
    "          loc='upper left')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "total = sex_disease.sum(axis=1)\n",
    "for i in range(len(total)):\n",
    "    plt.text(i, total[i]/2, str(total[i]), \n",
    "             ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques détaillées\n",
    "print(\"\\nStatistiques détaillées par sexe :\")\n",
    "print(\"-\" * 50)\n",
    "for sex in ['Femme', 'Homme']:\n",
    "    print(f\"\\nPour les {sex}s :\")\n",
    "    subset = df[df['sex_label'] == sex]\n",
    "    total = len(subset)\n",
    "    malades = len(subset[subset['target'] == 1])\n",
    "    pourcentage = (malades / total) * 100\n",
    "    print(f\"Nombre total : {total}\")\n",
    "    print(f\"Nombre de malades : {malades}\")\n",
    "    print(f\"Pourcentage de malades : {pourcentage:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifions la structure de nos données\n",
    "print(\"Structure de X :\")\n",
    "print(X.columns)\n",
    "print(\"\\nType de y :\")\n",
    "print(type(y))\n",
    "print(\"\\nContenu de y (premières valeurs) :\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du DataFrame combiné de manière plus robuste\n",
    "df = X.copy()\n",
    "df['target'] = y.values if hasattr(y, 'values') else y\n",
    "\n",
    "# Mappings pour les labels\n",
    "sex_mapping = {0: 'Femme', 1: 'Homme'}\n",
    "target_mapping = {0: 'Pas de maladie', 1: 'Maladie cardiaque'}\n",
    "cp_mapping = {\n",
    "    0: 'Angine typique',\n",
    "    1: 'Angine atypique',\n",
    "    2: 'Douleur non angineuse',\n",
    "    3: 'Asymptomatique'\n",
    "}\n",
    "\n",
    "# Création du graphique\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Création du tableau croisé\n",
    "cp_disease = pd.crosstab(df['cp'], df['target'])\n",
    "\n",
    "# Visualisation\n",
    "ax = cp_disease.plot(kind='bar', \n",
    "                    color=['#2ecc71', '#e74c3c'])\n",
    "\n",
    "plt.title('Types de douleur thoracique vs Maladie cardiaque',\n",
    "          fontsize=14, pad=20)\n",
    "plt.xlabel('Type de douleur thoracique', fontsize=12)\n",
    "plt.ylabel('Nombre de patients', fontsize=12)\n",
    "plt.legend(['Pas de maladie', 'Maladie cardiaque'],\n",
    "          title='Diagnostic')\n",
    "\n",
    "# Ajout des étiquettes plus explicatives\n",
    "plt.xticks(range(4), \n",
    "          ['Angine typique', 'Angine atypique', \n",
    "           'Douleur non angineuse', 'Asymptomatique'],\n",
    "          rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques détaillées avec gestion des cas où total = 0\n",
    "print(\"\\nStatistiques par type de douleur thoracique :\")\n",
    "print(\"-\" * 60)\n",
    "for cp_type in range(4):\n",
    "    subset = df[df['cp'] == cp_type]\n",
    "    total = len(subset)\n",
    "    \n",
    "    # Vérification si le sous-ensemble n'est pas vide\n",
    "    if total > 0:\n",
    "        malades = sum(subset['target'] == 1)\n",
    "        pourcentage = (malades / total) * 100\n",
    "        print(f\"\\nType {cp_type} ({cp_mapping[cp_type]}) :\")\n",
    "        print(f\"Nombre total de patients : {total}\")\n",
    "        print(f\"Nombre de patients malades : {malades}\")\n",
    "        print(f\"Pourcentage de malades : {pourcentage:.1f}%\")\n",
    "    else:\n",
    "        print(f\"\\nType {cp_type} ({cp_mapping[cp_type]}) :\")\n",
    "        print(\"Aucun patient dans cette catégorie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une figure pour les trois analyses\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 1. Pression artérielle\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxplot(x=df['target'].map({0:'Sain', 1:'Malade'}), \n",
    "            y=df['trestbps'], \n",
    "            palette=['#2ecc71', '#e74c3c'])\n",
    "plt.title('Pression artérielle selon le diagnostic')\n",
    "plt.xlabel('Diagnostic')\n",
    "plt.ylabel('Pression artérielle (mm Hg)')\n",
    "\n",
    "# 2. Cholestérol\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxplot(x=df['target'].map({0:'Sain', 1:'Malade'}), \n",
    "            y=df['chol'], \n",
    "            palette=['#2ecc71', '#e74c3c'])\n",
    "plt.title('Cholestérol selon le diagnostic')\n",
    "plt.xlabel('Diagnostic')\n",
    "plt.ylabel('Cholestérol (mg/dl)')\n",
    "\n",
    "# 3. Fréquence cardiaque maximale\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(x=df['target'].map({0:'Sain', 1:'Malade'}), \n",
    "            y=df['thalach'], \n",
    "            palette=['#2ecc71', '#e74c3c'])\n",
    "plt.title('Fréquence cardiaque max selon le diagnostic')\n",
    "plt.xlabel('Diagnostic')\n",
    "plt.ylabel('Fréquence cardiaque max')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques détaillées avec gestion d'erreurs\n",
    "print(\"\\nStatistiques détaillées par variable :\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "variables = {\n",
    "    'trestbps': 'Pression artérielle',\n",
    "    'chol': 'Cholestérol',\n",
    "    'thalach': 'Fréquence cardiaque max'\n",
    "}\n",
    "\n",
    "for var_code, var_name in variables.items():\n",
    "    print(f\"\\n{var_name} :\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for target in [0, 1]:\n",
    "        status = \"Patients sains\" if target == 0 else \"Patients malades\"\n",
    "        subset = df[df['target'] == target][var_code]\n",
    "        \n",
    "        print(f\"\\n{status} :\")\n",
    "        if not subset.empty:\n",
    "            print(f\"Nombre de patients : {len(subset)}\")\n",
    "            print(f\"Moyenne : {subset.mean():.2f}\")\n",
    "            print(f\"Médiane : {subset.median():.2f}\")\n",
    "            print(f\"Écart-type : {subset.std():.2f}\")\n",
    "            print(f\"Minimum : {subset.min():.2f}\")\n",
    "            print(f\"Maximum : {subset.max():.2f}\")\n",
    "        else:\n",
    "            print(\"Aucune donnée disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Création du graphique pour la glycémie à jeun\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Création du tableau croisé\n",
    "fbs_disease = pd.crosstab(df['fbs'], df['target'])\n",
    "\n",
    "# Visualisation avec des barres\n",
    "ax = fbs_disease.plot(kind='bar', color=['#2ecc71', '#e74c3c'])\n",
    "\n",
    "plt.title('Distribution des maladies cardiaques selon la glycémie à jeun', \n",
    "          fontsize=12, pad=20)\n",
    "plt.xlabel('Glycémie à jeun > 120 mg/dl', fontsize=10)\n",
    "plt.ylabel('Nombre de patients', fontsize=10)\n",
    "plt.legend(['Pas de maladie', 'Maladie cardiaque'], title='Diagnostic')\n",
    "plt.xticks([0, 1], ['Non (≤120)', 'Oui (>120)'], rotation=0)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques détaillées\n",
    "print(\"\\nAnalyse de la glycémie à jeun :\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for fbs_value in [0, 1]:\n",
    "    subset = df[df['fbs'] == fbs_value]\n",
    "    total = len(subset)\n",
    "    malades = sum(subset['target'] == 1)\n",
    "    \n",
    "    if total > 0:\n",
    "        pourcentage = (malades / total) * 100\n",
    "        status = \"≤120 mg/dl\" if fbs_value == 0 else \">120 mg/dl\"\n",
    "        print(f\"\\nPatients avec glycémie à jeun {status} :\")\n",
    "        print(f\"Nombre total de patients : {total}\")\n",
    "        print(f\"Nombre de patients malades : {malades}\")\n",
    "        print(f\"Pourcentage de malades : {pourcentage:.1f}%\")\n",
    "\n",
    "# Test statistique (Chi-2 car variables catégorielles)\n",
    "contingency_table = pd.crosstab(df['fbs'], df['target'])\n",
    "chi2, p_value = stats.chi2_contingency(contingency_table)[:2]\n",
    "\n",
    "print(\"\\nTest statistique (Chi-2) :\")\n",
    "print(f\"p-value : {p_value:.4f}\")\n",
    "print(f\"Association statistiquement significative : {'Oui' if p_value < 0.05 else 'Non'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du graphique pour l'angine induite par l'exercice\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Création du tableau croisé\n",
    "exang_disease = pd.crosstab(df['exang'], df['target'])\n",
    "\n",
    "# Visualisation avec des barres\n",
    "ax = exang_disease.plot(kind='bar', color=['#2ecc71', '#e74c3c'])\n",
    "\n",
    "plt.title('Distribution des maladies cardiaques selon l\\'angine induite par l\\'exercice', \n",
    "          fontsize=12, pad=20)\n",
    "plt.xlabel('Angine induite par l\\'exercice', fontsize=10)\n",
    "plt.ylabel('Nombre de patients', fontsize=10)\n",
    "plt.legend(['Pas de maladie', 'Maladie cardiaque'], title='Diagnostic')\n",
    "plt.xticks([0, 1], ['Non', 'Oui'], rotation=0)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques détaillées\n",
    "print(\"\\nAnalyse de l'angine induite par l'exercice :\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for exang_value in [0, 1]:\n",
    "    subset = df[df['exang'] == exang_value]\n",
    "    total = len(subset)\n",
    "    malades = sum(subset['target'] == 1)\n",
    "    \n",
    "    if total > 0:\n",
    "        pourcentage = (malades / total) * 100\n",
    "        status = \"sans angine d'effort\" if exang_value == 0 else \"avec angine d'effort\"\n",
    "        print(f\"\\nPatients {status} :\")\n",
    "        print(f\"Nombre total de patients : {total}\")\n",
    "        print(f\"Nombre de patients malades : {malades}\")\n",
    "        print(f\"Pourcentage de malades : {pourcentage:.1f}%\")\n",
    "\n",
    "# Test statistique Chi-2\n",
    "contingency_table = pd.crosstab(df['exang'], df['target'])\n",
    "chi2, p_value = stats.chi2_contingency(contingency_table)[:2]\n",
    "\n",
    "print(\"\\nTest statistique (Chi-2) :\")\n",
    "print(f\"p-value : {p_value:.4f}\")\n",
    "print(f\"Association statistiquement significative : {'Oui' if p_value < 0.05 else 'Non'}\")\n",
    "\n",
    "# Calcul du rapport de risque\n",
    "print(\"\\nRapport de risque :\")\n",
    "with_exang = (df[df['exang'] == 1]['target'] == 1).mean()\n",
    "without_exang = (df[df['exang'] == 0]['target'] == 1).mean()\n",
    "risk_ratio = with_exang / without_exang if without_exang > 0 else \"Non calculable\"\n",
    "print(f\"Les patients avec angine d'effort ont {risk_ratio:.2f} fois plus de risque d'avoir une maladie cardiaque\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import des bibliothèques\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 2. Définition des grilles de paramètres simplifiées\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "    },\n",
    "    'KNN': {\n",
    "        'classifier__n_neighbors': [3, 5, 7],\n",
    "        'classifier__weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [10, 20, None],\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. Fonction d'évaluation corrigée\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba=None):\n",
    "    results = {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'Recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'F1-Score': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# 4. Optimisation des modèles sélectionnés\n",
    "optimized_results = {}\n",
    "print(\"\\nOptimisation des modèles :\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name in ['Logistic Regression', 'KNN', 'Random Forest']:\n",
    "    print(f\"\\nOptimisation du modèle : {name}\")\n",
    "    \n",
    "    # Création du pipeline\n",
    "    pipeline = create_pipeline(models[name].named_steps['classifier'])\n",
    "    \n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grids[name],\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Entraînement\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluation du meilleur modèle\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    model_results = evaluate_model(y_test, y_pred)\n",
    "    optimized_results[name] = model_results\n",
    "    \n",
    "    print(f\"\\nMeilleurs paramètres pour {name}:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"Meilleur score CV: {grid_search.best_score_:.4f}\")\n",
    "    print(\"\\nPerformances sur l'ensemble de test:\")\n",
    "    for metric, value in model_results.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# 5. Affichage des résultats comparatifs\n",
    "results_df = pd.DataFrame(optimized_results).round(4)\n",
    "print(\"\\nComparaison des modèles optimisés :\")\n",
    "print(results_df)\n",
    "\n",
    "# 6. Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "results_df.plot(kind='bar', rot=45)\n",
    "plt.title('Comparaison des modèles optimisés')\n",
    "plt.xlabel('Métriques')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(title='Modèles', bbox_to_anchor=(1.05, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# Dans votre notebook\n",
    "best_model = LogisticRegression()  # Votre meilleur modèle optimisé\n",
    "joblib.dump(best_model, 'best_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans votre notebook de modélisation\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier  # On va utiliser Random Forest comme exemple\n",
    "\n",
    "# 1. Création du pipeline final\n",
    "final_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 2. Entraînement sur toutes les données\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# 3. Sauvegarde du scaler (après l'avoir ajusté sur toutes les données)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 4. Sauvegarde des fichiers\n",
    "# Sauvegarder le modèle\n",
    "pickle.dump(final_pipeline, open('model.pkl', 'wb'))\n",
    "# Sauvegarder le scaler\n",
    "pickle.dump(scaler, open('scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score sur l'ensemble d'entraînement : 0.983\n",
      "Score sur l'ensemble de test : 0.541\n",
      "\n",
      "Importance des features :\n",
      "     feature  importance\n",
      "7    thalach    0.147481\n",
      "11        ca    0.122218\n",
      "9    oldpeak    0.120803\n",
      "12      thal    0.120743\n",
      "4       chol    0.113473\n",
      "0        age    0.103226\n",
      "3   trestbps    0.094998\n",
      "2         cp    0.065881\n",
      "1        sex    0.034462\n",
      "8      exang    0.028527\n",
      "10     slope    0.020114\n",
      "6    restecg    0.014429\n",
      "5        fbs    0.013645\n"
     ]
    }
   ],
   "source": [
    "# Version corrigée de train_model.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pickle\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Chargement des données\n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "X = heart_disease.data.features \n",
    "y = heart_disease.data.targets.values.ravel()\n",
    "\n",
    "# Création du pipeline avec des paramètres ajustés pour réduire le surapprentissage\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,  # Réduit pour éviter le surapprentissage\n",
    "        max_depth=3,         # Réduit pour éviter le surapprentissage\n",
    "        min_samples_split=5, # Augmenté pour plus de généralisation\n",
    "        min_samples_leaf=3,  # Ajouté pour plus de généralisation\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Division des données\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Entraînement\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation\n",
    "train_score = pipeline.score(X_train, y_train)\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "print(f\"Score sur l'ensemble d'entraînement : {train_score:.3f}\")\n",
    "print(f\"Score sur l'ensemble de test : {test_score:.3f}\")\n",
    "\n",
    "# Importance des features\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': pipeline.named_steps['classifier'].feature_importances_\n",
    "})\n",
    "print(\"\\nImportance des features :\")\n",
    "print(feature_importance.sort_values('importance', ascending=False))\n",
    "\n",
    "# Sauvegarde du modèle uniquement\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(pipeline, file)\n",
    "\n",
    "# Stockage des features importantes dans une variable globale\n",
    "FEATURE_IMPORTANCE = feature_importance.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans votre notebook de modélisation\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier  # On va utiliser Random Forest comme exemple\n",
    "\n",
    "# 1. Création du pipeline final\n",
    "final_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 2. Entraînement sur toutes les données\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# 3. Sauvegarde du scaler (après l'avoir ajusté sur toutes les données)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 4. Sauvegarde des fichiers\n",
    "# Sauvegarder le modèle\n",
    "pickle.dump(final_pipeline, open('model.pkl', 'wb'))\n",
    "# Sauvegarder le scaler\n",
    "pickle.dump(scaler, open('scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "\n",
      "Évaluation des modèles avec validation croisée :\n",
      "--------------------------------------------------\n",
      "\n",
      "Entraînement de Logistic Regression...\n",
      "Scores CV (F1): 0.541 (+/- 0.087)\n",
      "\n",
      "Entraînement de KNN...\n",
      "Scores CV (F1): 0.550 (+/- 0.075)\n",
      "\n",
      "Entraînement de SVM...\n",
      "Scores CV (F1): 0.538 (+/- 0.019)\n",
      "\n",
      "Entraînement de Decision Tree...\n",
      "Scores CV (F1): 0.443 (+/- 0.069)\n",
      "\n",
      "Entraînement de Random Forest...\n",
      "Scores CV (F1): 0.550 (+/- 0.110)\n",
      "\n",
      "Entraînement de AdaBoost...\n",
      "Scores CV (F1): 0.528 (+/- 0.064)\n",
      "\n",
      "Résultats de l'évaluation finale :\n",
      "--------------------------------------------------\n",
      "           Logistic Regression    KNN    SVM  Decision Tree  Random Forest  \\\n",
      "Accuracy                 0.574  0.492  0.557          0.525          0.574   \n",
      "Precision                0.619  0.445  0.630          0.579          0.553   \n",
      "Recall                   0.574  0.492  0.557          0.525          0.574   \n",
      "F1-Score                 0.592  0.467  0.577          0.524          0.562   \n",
      "AUC-ROC                  0.782  0.671  0.810          0.722          0.782   \n",
      "\n",
      "           AdaBoost  \n",
      "Accuracy      0.508  \n",
      "Precision     0.455  \n",
      "Recall        0.508  \n",
      "F1-Score      0.480  \n",
      "AUC-ROC       0.728  \n",
      "\n",
      "Meilleur modèle (F1-Score) : Logistic Regression\n",
      "\n",
      "Paramètres du meilleur modèle :\n",
      "{'C': 1.0, 'class_weight': 'balanced', 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "Résultats sauvegardés dans 'model_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pickle\n",
    "\n",
    "# Chargement des données\n",
    "print(\"Chargement des données...\")\n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "X = heart_disease.data.features \n",
    "y = heart_disease.data.targets.values.ravel()\n",
    "\n",
    "# Division des données\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Paramètres optimisés pour Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 10,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features': 'sqrt',\n",
    "    'class_weight': 'balanced',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Création des pipelines avec paramètres optimisés\n",
    "models = {\n",
    "    \"Logistic Regression\": Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(C=1.0, class_weight='balanced', random_state=42))\n",
    "    ]),\n",
    "    \"KNN\": Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', KNeighborsClassifier(n_neighbors=5, weights='distance'))\n",
    "    ]),\n",
    "    \"SVM\": Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', SVC(probability=True, class_weight='balanced', random_state=42))\n",
    "    ]),\n",
    "    \"Decision Tree\": Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', DecisionTreeClassifier(max_depth=5, class_weight='balanced', random_state=42))\n",
    "    ]),\n",
    "    \"Random Forest\": Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier(**rf_params))\n",
    "    ]),\n",
    "    \"AdaBoost\": Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', AdaBoostClassifier(\n",
    "            estimator=DecisionTreeClassifier(max_depth=3),\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            algorithm='SAMME',\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_proba):\n",
    "    \"\"\"Évaluation avec gestion des cas limites\"\"\"\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        \"F1-Score\": f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        \"AUC-ROC\": roc_auc_score(y_true, y_proba, multi_class='ovr')\n",
    "    }\n",
    "\n",
    "# Entraînement et évaluation\n",
    "results = {}\n",
    "print(\"\\nÉvaluation des modèles avec validation croisée :\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, pipeline in models.items():\n",
    "    print(f\"\\nEntraînement de {name}...\")\n",
    "    \n",
    "    # Validation croisée\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1_weighted')\n",
    "    print(f\"Scores CV (F1): {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "    \n",
    "    # Entraînement sur l'ensemble complet\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_proba = pipeline.predict_proba(X_test)\n",
    "    \n",
    "    # Évaluation\n",
    "    results[name] = evaluate_model(y_test, y_pred, y_proba)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nRésultats de l'évaluation finale :\")\n",
    "print(\"-\" * 50)\n",
    "results_df = pd.DataFrame(results).round(3)\n",
    "print(results_df)\n",
    "\n",
    "# Identification du meilleur modèle\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['F1-Score'])[0]\n",
    "print(f\"\\nMeilleur modèle (F1-Score) : {best_model_name}\")\n",
    "\n",
    "# Sauvegarde du meilleur modèle\n",
    "best_model = models[best_model_name]\n",
    "with open('best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Affichage des paramètres du meilleur modèle\n",
    "print(f\"\\nParamètres du meilleur modèle :\")\n",
    "best_classifier = best_model.named_steps['classifier']\n",
    "print(best_classifier.get_params())\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "results_df.to_csv('model_results.csv')\n",
    "print(\"\\nRésultats sauvegardés dans 'model_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimisation de Logistic Regression...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Meilleur score pour Logistic Regression: 0.554\n",
      "Meilleurs paramètres: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
      "\n",
      "Optimisation de KNN...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Meilleur score pour KNN: 0.550\n",
      "Meilleurs paramètres: {'classifier__metric': 'euclidean', 'classifier__n_neighbors': 5, 'classifier__weights': 'distance'}\n",
      "\n",
      "Optimisation de Random Forest...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Meilleur score pour Random Forest: 0.577\n",
      "Meilleurs paramètres: {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 100}\n",
      "Sauvegarde du meilleur modèle...\n",
      "Modèle sauvegardé avec succès!\n",
      "\n",
      "Meilleur modèle après optimisation : Random Forest\n",
      "Score F1 : 0.577\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Définition des grilles de paramètres pour chaque modèle\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        'classifier__C': [0.01, 0.1, 1, 10],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [5, 10, None],\n",
    "        'classifier__min_samples_split': [2, 5],\n",
    "        'classifier__min_samples_leaf': [1, 2],\n",
    "        'classifier__max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Fonction d'optimisation\n",
    "def optimize_model(pipeline, param_grid, X, y):\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "# Optimisation des trois meilleurs modèles\n",
    "best_models = {}\n",
    "for name in [\"Logistic Regression\", \"KNN\", \"Random Forest\"]:\n",
    "    print(f\"\\nOptimisation de {name}...\")\n",
    "    best_model, best_params, best_score = optimize_model(\n",
    "        models[name],\n",
    "        param_grids[name],\n",
    "        X,\n",
    "        y\n",
    "    )\n",
    "    best_models[name] = {\n",
    "        'model': best_model,\n",
    "        'params': best_params,\n",
    "        'score': best_score\n",
    "    }\n",
    "    print(f\"Meilleur score pour {name}: {best_score:.3f}\")\n",
    "    print(f\"Meilleurs paramètres: {best_params}\")\n",
    "\n",
    "# Sélection du meilleur modèle optimisé\n",
    "best_model_name = max(best_models.items(), key=lambda x: x[1]['score'])[0]\n",
    "best_model = best_models[best_model_name]['model']\n",
    "\n",
    "# À la fin, assurez-vous que cette partie est exécutée\n",
    "print(\"Sauvegarde du meilleur modèle...\")\n",
    "with open('best_model_optimized.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "print(\"Modèle sauvegardé avec succès!\")\n",
    "\n",
    "print(f\"\\nMeilleur modèle après optimisation : {best_model_name}\")\n",
    "print(f\"Score F1 : {best_models[best_model_name]['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation du modèle...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "\n",
      "Meilleurs paramètres: {'classifier__class_weight': 'balanced', 'classifier__max_depth': 10, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 4, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 300}\n",
      "\n",
      "Rapport de classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91        33\n",
      "           1       0.44      0.36      0.40        11\n",
      "           2       0.20      0.14      0.17         7\n",
      "           3       0.23      0.43      0.30         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61        61\n",
      "   macro avg       0.36      0.36      0.35        61\n",
      "weighted avg       0.64      0.61      0.62        61\n",
      "\n",
      "\n",
      "Matrice de confusion:\n",
      "[[29  3  1  0  0]\n",
      " [ 2  4  0  4  1]\n",
      " [ 0  2  1  3  1]\n",
      " [ 0  0  3  3  1]\n",
      " [ 0  0  0  3  0]]\n",
      "\n",
      "Analyse des erreurs:\n",
      "Nombre d'erreurs: 24\n",
      "\n",
      "Importance des features:\n",
      "     feature  importance\n",
      "7    thalach    0.138227\n",
      "0        age    0.128431\n",
      "9    oldpeak    0.122359\n",
      "11        ca    0.109281\n",
      "4       chol    0.107533\n",
      "3   trestbps    0.093868\n",
      "12      thal    0.089189\n",
      "2         cp    0.065749\n",
      "8      exang    0.041045\n",
      "10     slope    0.040042\n",
      "6    restecg    0.029559\n",
      "1        sex    0.024786\n",
      "5        fbs    0.009932\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, precision_score, recall_score, f1_score\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pickle\n",
    "\n",
    "# Chargement des données\n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "X = heart_disease.data.features \n",
    "y = heart_disease.data.targets.values.ravel()\n",
    "\n",
    "# Division des données\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Création du pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Paramètres de recherche optimisés\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [200, 300],\n",
    "    'classifier__max_depth': [10, 15],\n",
    "    'classifier__min_samples_split': [5, 10],\n",
    "    'classifier__min_samples_leaf': [2, 4],\n",
    "    'classifier__class_weight': ['balanced'],\n",
    "    'classifier__max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "# Définition des métriques personnalisées\n",
    "scoring = {\n",
    "    'precision': make_scorer(precision_score, zero_division=0, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, zero_division=0, average='weighted'),\n",
    "    'f1': make_scorer(f1_score, zero_division=0, average='weighted')\n",
    "}\n",
    "\n",
    "# Recherche des meilleurs paramètres avec validation croisée\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=scoring,\n",
    "    refit='f1',  # Optimise pour le score F1\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Optimisation du modèle...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation du modèle\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nMeilleurs paramètres:\", grid_search.best_params_)\n",
    "print(\"\\nRapport de classification:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "print(\"\\nMatrice de confusion:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Calcul des probabilités sur l'ensemble de test\n",
    "y_proba = best_model.predict_proba(X_test)\n",
    "\n",
    "# Analyse des erreurs\n",
    "errors = X_test[y_test != y_pred]\n",
    "print(\"\\nAnalyse des erreurs:\")\n",
    "print(f\"Nombre d'erreurs: {len(errors)}\")\n",
    "\n",
    "# Importance des features\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': best_model.named_steps['classifier'].feature_importances_\n",
    "})\n",
    "print(\"\\nImportance des features:\")\n",
    "print(feature_importance.sort_values('importance', ascending=False))\n",
    "\n",
    "# Sauvegarde du modèle\n",
    "with open('best_model_optimized.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Sauvegarde des méta-informations\n",
    "model_info = {\n",
    "    'feature_importance': feature_importance.to_dict(),\n",
    "    'best_params': grid_search.best_params_,\n",
    "    'cv_results': grid_search.cv_results_\n",
    "}\n",
    "with open('model_info.pkl', 'wb') as file:\n",
    "    pickle.dump(model_info, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Entraînement du modèle...\n",
      "Évaluation du modèle...\n",
      "\n",
      "Rapport de classification :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89        33\n",
      "           1       0.84      0.93      0.88        28\n",
      "\n",
      "    accuracy                           0.89        61\n",
      "   macro avg       0.89      0.89      0.89        61\n",
      "weighted avg       0.89      0.89      0.89        61\n",
      "\n",
      "\n",
      "Matrice de confusion :\n",
      "[[28  5]\n",
      " [ 2 26]]\n",
      "\n",
      "Score AUC-ROC : 0.951\n",
      "\n",
      "Importance des features :\n",
      "     feature  importance\n",
      "12      thal    0.221454\n",
      "2         cp    0.153583\n",
      "11        ca    0.105273\n",
      "0        age    0.095498\n",
      "4       chol    0.091347\n",
      "7    thalach    0.084317\n",
      "3   trestbps    0.083937\n",
      "9    oldpeak    0.071607\n",
      "1        sex    0.041719\n",
      "10     slope    0.019401\n",
      "8      exang    0.016845\n",
      "6    restecg    0.009212\n",
      "5        fbs    0.005808\n",
      "\n",
      "Analyse des prédictions :\n",
      "\n",
      "Classe 0 (Sain):\n",
      "Nombre d'échantillons : 33\n",
      "Précision : 0.848\n",
      "Probabilité moyenne : 0.209\n",
      "Confiance (écart-type) : 0.340\n",
      "\n",
      "Classe 1 (Malade):\n",
      "Nombre d'échantillons : 28\n",
      "Précision : 0.929\n",
      "Probabilité moyenne : 0.940\n",
      "Confiance (écart-type) : 0.212\n",
      "\n",
      "Sauvegarde du modèle...\n",
      "Terminé!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pickle\n",
    "\n",
    "# Chargement des données\n",
    "print(\"Chargement des données...\")\n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "X = heart_disease.data.features \n",
    "# Conversion en classification binaire : 0 pour sain, 1 pour tout type de maladie\n",
    "y = (heart_disease.data.targets.values.ravel() > 0).astype(int)\n",
    "\n",
    "# Division des données\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Création des classificateurs\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline avec SMOTE\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('sampler', SMOTE(random_state=42)),\n",
    "    ('classifier', clf)\n",
    "])\n",
    "\n",
    "# Entraînement\n",
    "print(\"Entraînement du modèle...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation\n",
    "print(\"Évaluation du modèle...\")\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nRapport de classification :\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nMatrice de confusion :\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "print(f\"\\nScore AUC-ROC : {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "\n",
    "# Importance des features\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': pipeline.named_steps['classifier'].feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nImportance des features :\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Analyse détaillée\n",
    "print(\"\\nAnalyse des prédictions :\")\n",
    "for i in [0, 1]:\n",
    "    mask = y_test == i\n",
    "    if mask.any():\n",
    "        probas = y_proba[mask]\n",
    "        print(f\"\\nClasse {i} ({'Sain' if i == 0 else 'Malade'}):\")\n",
    "        print(f\"Nombre d'échantillons : {mask.sum()}\")\n",
    "        print(f\"Précision : {(y_pred[mask] == i).mean():.3f}\")\n",
    "        print(f\"Probabilité moyenne : {probas.mean():.3f}\")\n",
    "        print(f\"Confiance (écart-type) : {probas.std():.3f}\")\n",
    "\n",
    "# Sauvegarde\n",
    "print(\"\\nSauvegarde du modèle...\")\n",
    "with open('best_model_optimized.pkl', 'wb') as file:\n",
    "    pickle.dump(pipeline, file)\n",
    "\n",
    "model_info = {\n",
    "    'feature_importance': feature_importance.to_dict(),\n",
    "    'binary_conversion': True,\n",
    "    'feature_names': X.columns.tolist()\n",
    "}\n",
    "with open('model_info.pkl', 'wb') as file:\n",
    "    pickle.dump(model_info, file)\n",
    "\n",
    "print(\"Terminé!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
